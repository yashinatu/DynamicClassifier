{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Citations\n","#### Dataset\n","“Stratosphere Laboratory. A labeled dataset with malicious and benign IoT network traffic. January 22th. Agustin Parmisano, Sebastian Garcia, Maria Jose Erquiaga. https://www.stratosphereips.org/datasets-iot23\n","#### Model advices\n","Christian Desrosiers, École de Technologie Supérieur (ETS), for proposing XGBoost model.\n","#### Notebook creation\n","Rémi Blier, École de Technologie Supérieur (ETS), for creating this notebook.\n","#### XGBoost model\n","Chen, T., & Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 785–794). New York, NY, USA: ACM. https://doi.org/10.1145/2939672.2939785"]},{"cell_type":"markdown","metadata":{},"source":["### Import data"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-12-16T12:47:26.967561Z","iopub.status.busy":"2023-12-16T12:47:26.967097Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['CTU-IoT-Malware-Capture-1-1conn.log.labeled.csv']\n","archive\\CTU-IoT-Malware-Capture-1-1conn.log.labeled.csv\n","(1008748, 23)\n"]}],"source":["def find_csv_delimiter(file_path, max_lines=5):\n","    with open(file_path, 'r', newline='') as file:\n","        sample_lines = [file.readline().strip() for _ in range(max_lines)]\n","\n","    delimiters = [',', ';', '\\t', '|']  # Common delimiters to check\n","\n","    best_delimiter = ','\n","    max_delimiter_count = 0\n","\n","    for delimiter in delimiters:\n","        delimiter_count = sum(line.count(delimiter) for line in sample_lines)\n","        if delimiter_count > max_delimiter_count:\n","            best_delimiter = delimiter\n","            max_delimiter_count = delimiter_count\n","\n","    return best_delimiter\n","\n","import csv\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","\n","def extract_data_from_csv(file_path, delimiter=','):\n","    data = []  # Create a list to store the data\n","\n","    # Open the CSV file for reading\n","    with open(file_path, mode='r', newline='') as file:\n","        # Create a CSV reader object with the pipe delimiter\n","\n","        csv_reader = csv.reader(file, delimiter=delimiter)\n","\n","        # Read the header row\n","        header = next(csv_reader)\n","\n","        # Iterate through the rows in the CSV file\n","        for row in csv_reader:\n","            data.append(row)\n","    \n","    return data\n","\n","data = []\n","\n","import os\n","cpt = 0\n","directory = \"archive\"\n","print(os.listdir(directory))\n","for filename in os.listdir(directory):\n","    \n","        # for filename in filenames:\n","        if cpt < 2:\n","            file_path = os.path.join(directory, filename)\n","            delimiter = find_csv_delimiter(file_path)\n","            data += extract_data_from_csv(file_path, delimiter)\n","            print(os.path.join(directory, filename))\n","            cpt+=1\n","        else:\n","            break\n","\n"," \n","\n","# Convert your data to a NumPy array\n","data = np.array(data)\n","\n","print(data.shape)"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['1525879831.015811' 'CUmrqr4svHuSXJy5z7' '192.168.100.103' '51524'\n"," '65.127.233.163' '23' 'tcp' '-' '2.999051' '0' '0' 'S0' '-' '-' '0' 'S'\n"," '3' '180' '0' '0' '-' 'Malicious' 'PartOfAHorizontalPortScan']\n"]}],"source":["print(data[0])"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['192.168.100.103' '51524' '65.127.233.163' '23' 'tcp' '0' '0' '3' '180'\n"," '0' '0' 'Malicious']\n","(1008748, 12)\n","<class 'numpy.ndarray'>\n"]}],"source":["# columns_to_remove = [0, 1, 2, 4, 12, 13, 14, 20, 22]\n","columns_to_remove = [0, 1, 7, 8, 11, 12, 13, 14, 15, 20, 22]\n","data = np.delete(data, columns_to_remove, axis=1)\n","print(data[0])\n","print(data.shape)\n","print(type(data))"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(1008748, 18)\n"]}],"source":["# Make Malicious = 1 and Benign = 0\n","for row in data:\n","    if row[-1] == 'Benign':\n","        row[-1] = 0\n","    else:\n","        row[-1] = 1\n","\n","# Separating ip addresses as 4 numbers\n","def convert_ip_addresses(arr):\n","    new_data = []\n","    for i, item in enumerate(arr):\n","        if i == 0 or i == 2:\n","            ip_parts = item.split('.')\n","            new_data.extend(ip_parts if len(ip_parts) == 4 else ['0', '0', '0', '0'])\n","        else:\n","            new_data.append(item)\n","\n","    return np.array(new_data)\n","\n","new_data = []\n","\n","\n","for i in range(len(data)):\n","    new_data.append(convert_ip_addresses(data[i]))\n","\n","data = np.vstack(new_data)\n","print(data.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### Process data"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['192.0' '168.0' '100.0' '103.0' '51524.0' '65.0' '127.0' '233.0' '163.0'\n"," '23.0' 'tcp' '0.0' '0.0' '3.0' '180.0' '0.0' '0.0' '1.0']\n"]}],"source":["# Convert columns to int\n","new_data = []\n","for row in data:\n","    r = np.array([])\n","    for column in range(len(row)):\n","        if row[column] == '-':\n","            row[column] = 0\n","            r = np.append(r, 0)\n","        elif row[column] in ['tcp', 'udp', 'icmp']:\n","            r = np.append(r, row[column])\n","            continue\n","        else:\n","            try:\n","                # Attempt to convert the value to an integer\n","                r = np.append(r, row[column].astype(float))\n","            except ValueError:\n","                pass\n","    new_data.append(r)\n","\n","data = np.vstack(new_data)\n","print(data[0])\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'numpy.str_'>\n"]}],"source":["print(type(data[0][0]))"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['192.0' '168.0' '100.0' '103.0' '51524.0' '65.0' '127.0' '233.0' '163.0'\n"," '23.0' 'tcp' '0.0' '0.0' '3.0' '180.0' '0.0' '0.0' '1.0']\n","After one-hot encoding features:\n","['192.0' '168.0' '100.0' '103.0' '51524.0' '65.0' '127.0' '233.0' '163.0'\n"," '23.0' '0.0' '0.0' '3.0' '180.0' '0.0' '0.0' '1.0' True False]\n","(19,)\n"]}],"source":["import pandas as pd\n","import numpy as np\n","\n","print(data[0])\n","\n","# Assuming 'data' is your NumPy array\n","df = pd.DataFrame(data)\n","\n","# columns_to_onehot = [0, 1, 2, 3, 7, 8]\n","columns_to_onehot = [10]\n","\n","# for i in range(18):\n","#     columns_to_onehot.append(i+1)\n","#     print(type(data[0][i]))\n","\n","columns_to_encode = df.columns[columns_to_onehot]\n","\n","\n","\n","# Perform one-hot encoding\n","onehot_encoded = pd.get_dummies(df, columns=columns_to_encode, drop_first=True)\n","\n","# Display the result\n","print('After one-hot encoding features:')\n","print(onehot_encoded.values[0])\n","print(onehot_encoded.values[0].shape)\n","data = onehot_encoded.values\n"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[],"source":["# Check if any of the data contains strings\n","for row in data:\n","    for column in row:\n","        if isinstance(column, str):\n","            #Convert the value to a float, if possible\n","            try:\n","                column = float(column)\n","            except ValueError:\n","                pass\n","\n","# Initialize an empty list to store preprocessed data\n","preprocessed_data = []\n","\n","# Iterate through the rows in the data\n","for row in data:\n","    try:\n","        # Convert all columns to floats in this row\n","        float_row = [float(column) if column != '-' else 0.0 for column in row]\n","        preprocessed_data.append(float_row)\n","    except ValueError:\n","        print('Skipping row with non-convertible values:', row)\n","\n","data = preprocessed_data\n","\n","# Check if data contains strings\n","for row in data:\n","    for column in row:\n","        if isinstance(column, str):\n","            print('Error: String found in data: ', column)\n","            break\n","\n","preprocessed_data = data\n","\n","# Convert preprocessed_data to a normal Python list of lists\n","preprocessed_data = [list(row) for row in preprocessed_data]\n","\n","# # Print the preprocessed data\n","# for row in preprocessed_data:\n","#     print(row)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Separate data"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["import random\n","import numpy as np\n","\n","# Separate data\n","\n","# Define the split ratios for training, validation, and test datasets\n","train_ratio = 0.70  # 70% for training\n","val_ratio = 0.15   # 15% for validation\n","test_ratio = 0.15  # 15% for testing\n","\n","train_val_indices = int((train_ratio + val_ratio) * len(preprocessed_data))\n","\n","train_val_data = preprocessed_data[:train_val_indices]\n","test_data = preprocessed_data[train_val_indices:]\n","\n","# Shuffle the data randomly\n","random.shuffle(train_val_data)\n","random.shuffle(test_data)\n","\n","# Calculate the split points\n","total_records = len(train_val_data)\n","train_split = int(train_ratio * total_records)\n","val_split = int(val_ratio * total_records)\n","\n","# Split the data into training, validation\n","train_data = train_val_data[:train_split]\n","val_data = train_val_data[train_split:]\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[],"source":["train_labels = []\n","val_labels = []\n","test_labels = []\n","\n","# Separate the labels from features vectors\n","train_temp = []\n","for row in train_data:\n","    newRow = []\n","    newRow = row[:len(row)-1]\n","    train_temp.append(newRow)\n","    train_labels.append(row[-1])\n","    \n","val_temp = []\n","for row in val_data:\n","    newRow = []\n","    newRow = row[:len(row)-1]\n","    val_temp.append(newRow)\n","    val_labels.append(row[-1])\n","\n","test_temp = []\n","for row in test_data:\n","    newRow = []\n","    newRow = row[:len(row)-1]\n","    test_temp.append(newRow)\n","    test_labels.append(row[-1])\n","\n","train_data = train_temp\n","val_data = val_temp\n","test_data = test_temp"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(600204, 18)\n","(600204,)\n","(257231, 18)\n","(257231,)\n"]}],"source":["# Convert your data to NumPy arrays\n","train_data = np.array(train_data)\n","train_labels = np.array(train_labels)\n","\n","val_data = np.array(val_data)\n","val_labels = np.array(val_labels)\n","\n","test_data = np.array(test_data)\n","test_labels = np.array(test_labels)\n","\n","# Print rows and columns of the data\n","print(train_data.shape)\n","print(train_labels.shape)\n","print(val_data.shape)\n","print(val_labels.shape)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Train model"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[],"source":["\n","# Check if train data contains strings\n","for row in train_data:\n","    for column in row:\n","        if isinstance(column, str):\n","            print('Error: String found in data: ', column)\n","            break\n","\n","# Check if val data contains strings\n","for row in val_data:\n","    for column in row:\n","        if isinstance(column, str):\n","            print('Error: String found in data: ', column)\n","            break"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["0"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["import gc\n","gc.collect()"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"]}],"source":["from xgboost import XGBClassifier\n","from sklearn.model_selection import RandomizedSearchCV\n","import numpy as np\n","\n","# Define your XGBoost classifier and hyperparameter search space\n","xgb_model = XGBClassifier()\n","param_space = {\n","    'n_estimators': [100],\n","    'max_depth': [3, 4, 5, 6],\n","    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n","    'subsample': [0.8, 0.9, 1.0],\n","    'colsample_bytree': [0.8, 0.9, 1.0],\n","    'gamma': [0, 0.1, 0.2, 0.3],\n","}\n","\n","# Create a RandomizedSearchCV object\n","random_search = RandomizedSearchCV(\n","    xgb_model,\n","    param_space,\n","    n_iter=5,  # Adjust the number of iterations as needed\n","    scoring='accuracy',  # Use the appropriate scoring metric\n","    n_jobs=-1,  # Use all available CPU cores for parallel processing\n","    cv=5,  # Number of cross-validation folds\n","    random_state=42,  # Set a random seed for reproducibility\n","    verbose=3\n",")\n","\n","# Perform hyperparameter optimization\n","random_search.fit(train_data, train_labels)\n","\n","# Get the best hyperparameters and the best model\n","best_xgb_hps = random_search.best_params_\n","best_xgb_model = random_search.best_estimator_\n"]},{"cell_type":"code","execution_count":15,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'subsample': 0.8, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.01, 'gamma': 0.2, 'colsample_bytree': 0.8}\n"]}],"source":["print(best_xgb_hps)"]},{"cell_type":"markdown","metadata":{},"source":["### Evaluate model on val set"]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation Accuracy (Random Forest): 1.0\n","Validation Classification Report (XGBoost):\n","              precision    recall  f1-score   support\n","\n","         0.0       1.00      1.00      1.00    154492\n","         1.0       1.00      1.00      1.00    102739\n","\n","    accuracy                           1.00    257231\n","   macro avg       1.00      1.00      1.00    257231\n","weighted avg       1.00      1.00      1.00    257231\n","\n","Validation Confusion Matrix (XGBoost):\n","[[154492      0]\n"," [     0 102739]]\n"]}],"source":["from sklearn.metrics import classification_report, confusion_matrix\n","# Evaluate the Random Forest model on the validation data\n","sgboost_val_predictions = random_search.predict(val_data)\n","sgboost_val_accuracy = np.mean(sgboost_val_predictions == val_labels)\n","print(\"Validation Accuracy (Random Forest):\", sgboost_val_accuracy)\n","\n","# Calculate and print classification report and confusion matrix for Random Forest\n","sgboost_val_report = classification_report(val_labels, sgboost_val_predictions)\n","sgboost_val_confusion = confusion_matrix(val_labels, sgboost_val_predictions)\n","print(\"Validation Classification Report (XGBoost):\")\n","print(sgboost_val_report)\n","print(\"Validation Confusion Matrix (XGBoost):\")\n","print(sgboost_val_confusion)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Evaluate model on test set"]},{"cell_type":"code","execution_count":17,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy (SGBoost): 1.0\n","Test Classification Report (XGBoost):\n","              precision    recall  f1-score   support\n","\n","         0.0       1.00      1.00      1.00     85934\n","         1.0       1.00      1.00      1.00     65379\n","\n","    accuracy                           1.00    151313\n","   macro avg       1.00      1.00      1.00    151313\n","weighted avg       1.00      1.00      1.00    151313\n","\n","Test Confusion Matrix (XGBoost):\n","[[85934     0]\n"," [    0 65379]]\n"]}],"source":["# Evaluate the Random Forest model on the test data\n","sgboost_test_predictions = random_search.predict(test_data)\n","sgboost_test_accuracy = np.mean(sgboost_test_predictions == test_labels)\n","print(\"Test Accuracy (SGBoost):\", sgboost_test_accuracy)\n","\n","# Calculate and print classification report and confusion matrix for Random Forest\n","sgboost_test_report = classification_report(test_labels, sgboost_test_predictions)\n","sgboost_test_confusion = confusion_matrix(test_labels, sgboost_test_predictions)\n","print(\"Test Classification Report (XGBoost):\")\n","print(sgboost_test_report)\n","print(\"Test Confusion Matrix (XGBoost):\")\n","print(sgboost_test_confusion)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["import joblib\n","joblib.dump(best_xgb_model, 'best_xgb_model.joblib')\n","# If you want to save the best hyperparameters as well\n","with open('best_xgb_hyperparameters.txt', 'w') as file:\n","    file.write(str(best_xgb_hps))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the best XGBoost model from the file\n","loaded_xgb_model = joblib.load('best_xgb_model.joblib')\n","\n","# If you want to load the best hyperparameters as well\n","with open('best_xgb_hyperparameters.txt', 'r') as file:\n","    loaded_xgb_hps = eval(file.read())"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":3843877,"sourceId":6906778,"sourceType":"datasetVersion"}],"dockerImageVersionId":30558,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":4}
